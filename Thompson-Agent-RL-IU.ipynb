{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i-UF44JQuuW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Open this [link](https://rl-lab.com/multi-armed-bandits/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kgc_nCo10vHN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod, abstractproperty\n",
    "import enum\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzpz05vH0vHP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1. Bernoulli Bandit\n",
    "\n",
    "We are going to implement several exploration strategies for simplest problem - bernoulli bandit.\n",
    "\n",
    "The bandit has $K$ actions. Action produce 1.0 reward $r$ with probability $0 \\le \\theta_k \\le 1$ which is unknown to agent, but fixed over time. Agent's objective is to minimize regret over fixed number $T$ of action selections:\n",
    "\n",
    "$$\\rho = T\\theta^* - \\sum_{t=1}^T r_t$$\n",
    "\n",
    "Where $\\theta^* = \\max_k\\{\\theta_k\\}$\n",
    "\n",
    "**Real-world analogy:**\n",
    "\n",
    "Clinical trials - we have $K$ pills and $T$ ill patient. After taking pill, patient is cured with probability $\\theta_k$. Task is to find most efficient pill.\n",
    "\n",
    "A research on clinical trials - https://arxiv.org/pdf/1507.08025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m0p3R44Z0vHQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BernoulliBandit:\n",
    "    def __init__(self, n_actions=5):\n",
    "        self._probs = np.random.random(n_actions)\n",
    "\n",
    "    @property\n",
    "    def action_count(self):\n",
    "        return len(self._probs)\n",
    "\n",
    "    def pull(self, action):\n",
    "        if np.random.random() > self._probs[action]:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "\n",
    "    def optimal_reward(self):\n",
    "        \"\"\" Used for regret calculation\n",
    "        \"\"\"\n",
    "        return np.max(self._probs)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" \n",
    "        Used in nonstationary version\n",
    "        \"\"\"\n",
    "        self._probs += np.random.normal(0, 0.01, self.action_count)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" \n",
    "        Used in nonstationary version\n",
    "        \"\"\"\n",
    "        self._probs = np.random.random(self.action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BzQGvXXH0vHQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AbstractAgent(metaclass=ABCMeta):\n",
    "    def init_actions(self, n_actions):\n",
    "        self._successes = np.zeros(n_actions)\n",
    "        self._failures = np.zeros(n_actions)\n",
    "        self._total_pulls = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Get current best action\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Observe reward from action and update agent's internal parameters\n",
    "        :type action: int\n",
    "        :type reward: int\n",
    "        \"\"\"\n",
    "        self._total_pulls += 1\n",
    "        if reward == 1:\n",
    "            self._successes[action] += 1\n",
    "        else:\n",
    "            self._failures[action] += 1\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class RandomAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        return np.random.randint(0, len(self._successes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytpRyo_A0vHR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Epsilon-greedy agent\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat\\theta_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k)$\n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    "\n",
    "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$ with probability $1 - \\epsilon$ or random action with probability $\\epsilon$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "Implement the algorithm above in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i6cDYFBO0vHR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(AbstractAgent):\n",
    "    def __init__(self, epsilon=0.01):\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def get_action(self):\n",
    "        if np.random.random() < self._epsilon:\n",
    "            return np.random.randint(len(self._successes))\n",
    "        else:\n",
    "            return np.argmax(self._successes / (self._successes + self._failures + 0.1))\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__ + \"(epsilon={})\".format(self._epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzqWr5rT0vHR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### UCB Agent\n",
    "Epsilon-greedy strategy have no preference for actions. It would be better to select among actions that are uncertain or have potential to be optimal. One can come up with idea of index for each action that represents optimality and uncertainty at the same time. One efficient way to do it is to use UCB1 algorithm:\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k) + \\sqrt{2\\log(t) \\ / \\ (\\alpha_k + \\beta_k)}$\n",
    "\n",
    "&nbsp;&nbsp; **end for**\n",
    "\n",
    "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}w$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "__Note:__ in practice, one can multiply $\\sqrt{2\\log(t) \\ / \\ (\\alpha_k + \\beta_k)}$ by some tunable parameter to regulate agent's optimism and wilingness to abandon non-promising actions.\n",
    "\n",
    "More versions and optimality analysis - https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-Jx8a3Rd0vHS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class UCBAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        pulls = self._successes + self._failures + 0.1\n",
    "        return np.argmax(self._successes / pulls + np.sqrt(2 * np.log(self._total_pulls + 0.1) / pulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVhiTQx_0vHS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Homework\n",
    "Your assignment is to solve the Bernoulli Bandit problem using Thompson sampling algorithm.\n",
    "\n",
    "1- Based on your understanding write down the pseudo-code for Thompson sampling algorithm.\n",
    "\n",
    "2- Implement the `get_action(self)` in the cell below.\n",
    "\n",
    "3- Explain in detail the results you got in comparison with UCB and $\\epsilon$-Greedy.\n",
    "\n",
    "\n",
    "**Note:** Your grade will be based on your understanding of the algorithm and your analysis to the results. Cheating will be punished by 50% deduction for the first time and will get 0 in the second time. \n",
    "\n",
    "### Thompson sampling\n",
    "Thompson Sampling (also sometimes referred to as the Bayesian Bandits algorithm) takes a slightly different approach; rather than just refining an estimate of the mean reward it extends this, to instead build up a probability model from the obtained rewards, and then samples from this to choose an action. \n",
    "\n",
    "In this way, not only is an increasingly accurate estimate of the possible reward obtained, but the model also provides a level of confidence in this reward, and this confidence increases as more samples are collected. This process of updating your beliefs as more evidence becomes available is known as Bayesian Inference.\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "#estimate model:\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{\\theta}_{k} \\leftarrow {\\alpha}_{k} / ({\\alpha}_{k}+ {\\beta}_{k} )$\n",
    "\n",
    "&nbsp;&nbsp; **end for**\n",
    "\n",
    "&nbsp;&nbsp; #select and apply action:\n",
    "\n",
    "&nbsp;&nbsp; ${x}_{t} \\leftarrow {argmax}_{k} \\hat{\\theta}_{k}$\n",
    "\n",
    "&nbsp;&nbsp; Apply ${x}_{t}$ and observe ${r}_{t}$\n",
    "\n",
    "&nbsp;&nbsp; #update distribution:\n",
    " \n",
    "&nbsp;&nbsp; $(\\alpha_{x_{t}},\\beta_{x_{t}}) \\leftarrow (\\alpha_{x_{t}} + {r}_{t}, \\beta_{x_{t}} + 1 - {r}_{t})$\n",
    "\n",
    "**end for**\n",
    "\n",
    "\n",
    "\n",
    "More on Thompson Sampling:\n",
    "https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5Yl81VcG0vHT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        return np.argmax(np.random.beta(self._successes + 1, self._failures + 1))\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RIJTr4j10vHT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def get_regret(env, agents, n_steps=5000, n_trials=50):\n",
    "    scores = OrderedDict({\n",
    "        agent.name: [0.0 for step in range(n_steps)] for agent in agents\n",
    "    })\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        env.reset()\n",
    "\n",
    "        for a in agents:\n",
    "            a.init_actions(env.action_count)\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            optimal_reward = env.optimal_reward()\n",
    "\n",
    "            for agent in agents:\n",
    "                action = agent.get_action()\n",
    "                reward = env.pull(action)\n",
    "                agent.update(action, reward)\n",
    "                scores[agent.name][i] += optimal_reward - reward\n",
    "\n",
    "            env.step()  # change bandit's state if it is unstationary\n",
    "\n",
    "    for agent in agents:\n",
    "        scores[agent.name] = np.cumsum(scores[agent.name]) / n_trials\n",
    "\n",
    "    return scores\n",
    "\n",
    "def plot_regret(agents, scores):\n",
    "    for agent in agents:\n",
    "        plt.plot(scores[agent.name])\n",
    "\n",
    "    plt.legend([agent.name for agent in agents])\n",
    "\n",
    "    plt.ylabel(\"regret\")\n",
    "    plt.xlabel(\"steps\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "s2uISH8y0vHT",
    "outputId": "9f76477b-9758-446d-d245-bafb294fbca0",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment agents\n",
    "agents = [\n",
    "    EpsilonGreedyAgent(),\n",
    "    UCBAgent(),\n",
    "    ThompsonSamplingAgent()\n",
    "]\n",
    "\n",
    "regret = get_regret(BernoulliBandit(), agents, n_steps=10000, n_trials=10)\n",
    "plot_regret(agents, regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Algorithms that solve the bandit problem need to find a way to balance the trade-off between exploitation and exploration. They need to look for the best actions to take while at the same time trying to make use of the information they've already gained.\n",
    "\n",
    "In simple approaches, such as Epsilon-Greedy, this trade-off is achieved by mainly using the action that currently gives the most reward and adding simple exploration by now and again randomly trying some of the other actions. In more complex solutions, such as UCB, again the actions with the highest mean reward are selected most often but this is balanced by a confidence measure. This ensures that actions that have not been selected often will get tested.\n",
    "\n",
    "Thompson Sampling takes a different approach to these other methods. Instead of simply maintaining an estimate of the reward, it gradually refines a model of the probability of the reward for each action and actions are chosen by sampling from this distribution. It is therefore possible to get an estimate for the mean reward value of an action, plus a measure of confidence for that estimate. As we saw in our experiments, this allows it to quickly locate and lock onto the optimal action, to give a near optimal accumulated return."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3727cd8ee745de9c1bbf260b284704cc4d66220ef29216d307c0ae8700210bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}